{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomHorizontalFlip, Resize\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_images(base_path):\n",
    "    # Define the categories of the folders containing the images and their labels\n",
    "    categories = ['train', 'train_labels', 'val', 'val_labels', 'test', 'test_labels']\n",
    "    image_counts = {}\n",
    "    \n",
    "    # Iterate over each category to count the image files\n",
    "    for category in categories:\n",
    "        # Build the full path to the category folder\n",
    "        path = os.path.join(base_path, category)\n",
    "        # Count and store the number of files in each category\n",
    "        image_counts[category] = len([name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))])\n",
    "    \n",
    "    # Print a summary of the image count for each folder\n",
    "    print(\"Image count per folder:\", image_counts)\n",
    "    \n",
    "    # Categories containing images for display\n",
    "    sample_images = ['train', 'val', 'test']\n",
    "    for category in sample_images:\n",
    "        # Get the list of image files and labels\n",
    "        images_list = os.listdir(os.path.join(base_path, category))\n",
    "        labels_list = os.listdir(os.path.join(base_path, f\"{category}_labels\"))\n",
    "        \n",
    "        # Check if the image and label lists are not empty\n",
    "        if images_list and labels_list:\n",
    "            # Select the first image and label from the lists for display\n",
    "            image_path = os.path.join(base_path, category, images_list[0])\n",
    "            label_path = os.path.join(base_path, f\"{category}_labels\", labels_list[0])\n",
    "            \n",
    "            # Open the image and label with Pillow\n",
    "            image = Image.open(image_path)\n",
    "            label = Image.open(label_path)\n",
    "            \n",
    "            # Set up the display environment with Matplotlib\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            ax[0].imshow(image)\n",
    "            ax[0].set_title(f'Satellite Image ({category})')\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            ax[1].imshow(label, cmap='gray')\n",
    "            ax[1].set_title(f'Building Mask ({category})')\n",
    "            ax[1].axis('off')\n",
    "            \n",
    "            # Display the images and labels\n",
    "            plt.show()\n",
    "\n",
    "# Get the current directory and use it to call the function\n",
    "current_directory = os.getcwd()\n",
    "explore_images(os.path.join(current_directory, 'MBDS/png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteBuildingDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform_image=None, transform_mask=None):\n",
    "        \"\"\"\n",
    "        Initializes the SatelliteBuildingDataset class with specific directories for images and masks,\n",
    "        and with optional transformations that can be applied to both images and masks.\n",
    "\n",
    "        Args:\n",
    "            image_dir (string): Directory containing all images.\n",
    "            mask_dir (string): Directory containing all masks.\n",
    "            transform_image (callable, optional): Optional transformation function to apply to images.\n",
    "            transform_mask (callable, optional): Optional transformation function to apply to masks.\n",
    "        \"\"\"\n",
    "        # Assign to instance variables the values of the parameters\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform_image = transform_image\n",
    "        self.transform_mask = transform_mask\n",
    "        # List all files in the image directory. It is assumed that each image has its corresponding mask.\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the total number of images in the dataset\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Build the full paths for the image and mask using the provided index\n",
    "        img_name = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_name = os.path.join(self.mask_dir, self.images[idx])\n",
    "        \n",
    "        # Open the image and convert it to RGB color\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        # Open the mask and convert it to grayscale so that it has a single channel\n",
    "        mask = Image.open(mask_name).convert(\"L\")  \n",
    "\n",
    "        # Apply the specified transformations to the image and mask, if provided\n",
    "        if self.transform_image:\n",
    "            image = self.transform_image(image)\n",
    "        if self.transform_mask:\n",
    "            mask = self.transform_mask(mask)\n",
    "\n",
    "        # Return the processed image and mask\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(base_path, resize, batch_size=4):\n",
    "    \"\"\"\n",
    "    Creates and returns dataloaders for training and validation datasets.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Base path where the data folders are located.\n",
    "        resize (tuple): Dimensions to which all images and masks will be resized.\n",
    "        batch_size (int, optional): Number of samples per batch. The default value is 4.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define transformations to apply to the images:\n",
    "    # - Resize the images to the specified size.\n",
    "    # - Convert the images to PyTorch tensors.\n",
    "    # - Normalize the images using specified means and standard deviations.\n",
    "    transform_image = Compose([\n",
    "        Resize(resize),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Define transformations for the masks, which include resizing and conversion to tensor.\n",
    "    # Masks generally do not require normalization.\n",
    "    transform_mask = Compose([\n",
    "        Resize(resize),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Build full paths to the training and validation image and mask directories\n",
    "    train_dir = os.path.join(base_path, 'train')\n",
    "    train_mask_dir = os.path.join(base_path, 'train_labels')\n",
    "    val_dir = os.path.join(base_path, 'val')\n",
    "    val_mask_dir = os.path.join(base_path, 'val_labels')\n",
    "    \n",
    "    # Create the training and validation datasets using the SatelliteBuildingDataset class\n",
    "    train_dataset = SatelliteBuildingDataset(train_dir, train_mask_dir, transform_image, transform_mask)\n",
    "    val_dataset = SatelliteBuildingDataset(val_dir, val_mask_dir, transform_image, transform_mask)\n",
    "\n",
    "    # Create dataloaders for the training and validation datasets.\n",
    "    # The training dataloader shuffles the data (shuffle=True) to improve training.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Return the training and validation dataloaders\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you are running this from the main directory of your project\n",
    "base_path = os.getcwd()\n",
    "base_path = os.path.join(base_path, 'MBDS/png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model, as well as the loss function. \n",
    "# The model is of the U-Net type, with 4 encoder downsampling blocks and 4 decoder upsampling blocks.\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Double convolution block that applies two convolution layers with a batch normalization \n",
    "        and ReLU activation between them.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "        \"\"\"\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "    \n",
    "    \n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        DownBlock in U-Net that includes a DoubleConv followed by max pooling \n",
    "        to reduce spatial dimension.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels after DoubleConv.\n",
    "        \"\"\"\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "        self.down_sample = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_out = self.double_conv(x)\n",
    "        down_out = self.down_sample(skip_out)\n",
    "        return (down_out, skip_out)\n",
    "\n",
    "    \n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
    "        \"\"\"\n",
    "        UpBlock in U-Net that performs upsampling and combines features\n",
    "        from earlier layers through concatenation operation.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Total number of input channels (sum of output channels from previous block and skip connection).\n",
    "            out_channels (int): Number of output channels.\n",
    "            up_sample_mode (str): Upsampling mode, 'conv_transpose' for ConvTranspose2d or 'bilinear' for bilinear interpolation.\n",
    "        \"\"\"\n",
    "        super(UpBlock, self).__init__()\n",
    "        if up_sample_mode == 'conv_transpose':\n",
    "            self.up_sample = nn.ConvTranspose2d(in_channels - out_channels, in_channels - out_channels, kernel_size=2, stride=2)        \n",
    "        elif up_sample_mode == 'bilinear':\n",
    "            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, down_input, skip_input):\n",
    "        x = self.up_sample(down_input)\n",
    "        x = torch.cat([x, skip_input], dim=1)\n",
    "        return self.double_conv(x)\n",
    "\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_classes=1, up_sample_mode='conv_transpose'):\n",
    "        \"\"\"\n",
    "        U-Net architecture for segmentation that includes descending and ascending paths with a bottleneck.\n",
    "\n",
    "        Args:\n",
    "            out_classes (int): Number of output classes (usually 1 for binary segmentation masks).\n",
    "            up_sample_mode (str): Upsampling mode used in the ascent blocks.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        self.up_sample_mode = up_sample_mode\n",
    "        self.down_conv1 = DownBlock(3, 64)\n",
    "        self.down_conv2 = DownBlock(64, 128)\n",
    "        self.down_conv3 = DownBlock(128, 256)\n",
    "        self.down_conv4 = DownBlock(256, 512)\n",
    "        self.double_conv = DoubleConv(512, 1024)\n",
    "        self.up_conv4 = UpBlock(1024 + 512, 512, self.up_sample_mode)\n",
    "        self.up_conv3 = UpBlock(512 + 256, 256, self.up_sample_mode)\n",
    "        self.up_conv2 = UpBlock(256 + 128, 128, self.up_sample_mode)\n",
    "        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n",
    "        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip1_out = self.down_conv1(x)\n",
    "        x, skip2_out = self.down_conv2(x)\n",
    "        x, skip3_out = self.down_conv3(x)\n",
    "        x, skip4_out = self.down_conv4(x)\n",
    "        x = self.double_conv(x)\n",
    "        x = self.up_conv4(x, skip4_out)\n",
    "        x = self.up_conv3(x, skip3_out)\n",
    "        x = self.up_conv2(x, skip2_out)\n",
    "        x = self.up_conv1(x, skip1_out)\n",
    "        x = self.conv_last(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.):\n",
    "        \"\"\"\n",
    "        Implementation of the Dice Loss, useful for comparing the similarity between two samples.\n",
    "\n",
    "        Args:\n",
    "            smooth (float): Value to prevent division by zero and smooth the result.\n",
    "        \"\"\"\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs)  # Convert logits to probabilities\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)  \n",
    "        \n",
    "        return 1 - dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = UNet()\n",
    "# Check if CUDA is available and move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = DiceLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to ensure everything is functioning correctly\n",
    "# Create an input tensor and move it to the same device\n",
    "input_tensor = torch.rand((1, 3, 256, 256)).to(device)\n",
    "\n",
    "# Perform the prediction\n",
    "output_mask = model(input_tensor)\n",
    "print(output_mask.shape)  # The output shape should be [1, 1, 256, 256], corresponding to the predicted mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the IoU as a metric to see how the model progresses and performs. \n",
    "# The training metric is Dice, but it's good to have more than one metric if possible.\n",
    "\n",
    "def calculate_iou(outputs, masks):\n",
    "    \"\"\"\n",
    "    Calcula el promedio de la métrica Intersección sobre Unión (IoU) entre las salidas del modelo y las máscaras verdaderas.\n",
    "\n",
    "    Args:\n",
    "        outputs: Salidas del modelo (logits).\n",
    "        masks: Máscaras verdaderas (ground truth).\n",
    "\n",
    "    Returns:\n",
    "        Un valor flotante que representa el IoU promedio.\n",
    "    \"\"\"\n",
    "    outputs = torch.sigmoid(outputs) > 0.5  # Convertir logits a predicciones binarias\n",
    "    outputs = outputs.cpu().numpy().astype(np.uint8)  # Convertir a NumPy para cálculo\n",
    "    masks = masks.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # Calcular IoU para cada par de salida y máscara\n",
    "    ious = [jaccard_score(m.flatten(), o.flatten()) for m, o in zip(masks, outputs)]\n",
    "    return np.mean(ious)\n",
    "\n",
    "def weight_reset(m):\n",
    "    \"\"\"\n",
    "    Reinicia los pesos de las capas convolucionales y lineales a sus valores iniciales.\n",
    "\n",
    "    Args:\n",
    "        m: Módulo de PyTorch (capa).\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Input sizes to test\n",
    "sizes = [(224, 224), ]\n",
    "# Number of epochs for training\n",
    "num_epochs = 2\n",
    "# Get the base path of the project\n",
    "base_path = os.getcwd()\n",
    "# Path to images\n",
    "base_path = os.path.join(base_path, 'MBDS/png')\n",
    "# Initialize the best loss as infinity\n",
    "best_loss = float('inf')\n",
    "\n",
    "for resize in sizes:\n",
    "    print(f\"Training model with image size: {resize}\")\n",
    "    # Create dataloaders for training and validation\n",
    "    train_loader, val_loader = create_dataloaders(base_path, resize, batch_size=4)\n",
    "\n",
    "    # Initialize lists to store losses and IoU scores\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    iou_scores = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        train_tqdm = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} Training', leave=True)\n",
    "        for images, masks in train_tqdm:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # Reset optimizer gradients\n",
    "            outputs = model(images)  # Pass images through the model\n",
    "            loss = criterion(outputs, masks)  # Calculate loss\n",
    "            loss.backward()  # Backpropagate the error\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)  # Accumulate scaled loss by batch size\n",
    "            train_tqdm.set_postfix(loss=(running_loss / (train_tqdm.last_print_n + 1)))  # Update tqdm info\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)  # Calculate average loss for the epoch\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation process to evaluate the model\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_running_loss = 0.0\n",
    "        ious = []\n",
    "        val_tqdm = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} Validation', leave=True)\n",
    "        with torch.no_grad():  # Disable gradient computation for validation\n",
    "            for images, masks in val_tqdm:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                iou = calculate_iou(outputs, masks)  # Calculate IoU\n",
    "                ious.append(iou)\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(val_loader.dataset)  # Average validation loss\n",
    "        average_iou = np.mean(ious)  # Average IoU\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        iou_scores.append(average_iou)\n",
    "        \n",
    "        # Print training and validation statistics\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Average IoU: {iou_scores[-1]:.4f}')\n",
    "\n",
    "        # Save the model if the validation loss is the lowest so far\n",
    "        if val_epoch_loss < best_loss:\n",
    "            model_save_path = os.path.join('saved_models', f'model_{resize[0]}x{resize[1]}.pth')\n",
    "            os.makedirs('saved_models', exist_ok=True)\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved to {model_save_path}\")\n",
    "            best_loss = val_epoch_loss\n",
    "\n",
    "    # Visualize graphs of losses and IoU\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(f'Losses over Epochs for Resize: {resize}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(iou_scores, label='IoU Score')\n",
    "    plt.title(f'IoU Scores over Epochs for Resize: {resize}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('IoU Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model.apply(weight_reset)  # Reset model weights for the next size configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we evaluate the model on the test set. \n",
    "# Besides the metrics, I think it's useful to see some examples where the original image, the true mask, and the predicted mask are displayed.\n",
    "\n",
    "def create_test_dataloader(base_path, batch_size, resize):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for the test dataset.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Base path where images and masks are stored.\n",
    "        batch_size (int): Batch size.\n",
    "        resize (tuple): Dimensions to which images and masks will be resized.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: DataLoader object configured for the test dataset.\n",
    "    \"\"\"\n",
    "    # Transformations applied to the images\n",
    "    transform_image = Compose([\n",
    "        Resize(resize),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Transformations applied to the masks (only resizing and tensor conversion)\n",
    "    transform_mask = Compose([\n",
    "        Resize(resize),\n",
    "        ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Create the dataset using the specified directories\n",
    "    test_dir = os.path.join(base_path, 'test')\n",
    "    test_mask_dir = os.path.join(base_path, 'test_labels')\n",
    "    test_dataset = SatelliteBuildingDataset(test_dir, test_mask_dir, transform_image, transform_mask)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "def plot_examples(data_loader, model, device, num_examples=5):\n",
    "    \"\"\"\n",
    "    Displays examples of images, their true masks, and model predictions.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): DataLoader containing the test dataset.\n",
    "        model (nn.Module): Neural network model to generate predictions.\n",
    "        device (torch.device): Device where the model is running (CPU or GPU).\n",
    "        num_examples (int): Number of examples to visualize.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(num_examples, 3, figsize=(15, num_examples * 5))\n",
    "    model.eval()\n",
    "\n",
    "    for idx, (images, masks) in enumerate(data_loader):\n",
    "        if idx >= num_examples:\n",
    "            break\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs) > 0.5  # Convert logits to binary predictions\n",
    "\n",
    "        # Visualize the original image, the true mask, and the predicted mask\n",
    "        axs[idx, 0].imshow(images[0].cpu().permute(1, 2, 0))\n",
    "        axs[idx, 0].set_title('Original Image')\n",
    "        axs[idx, 0].axis('off')\n",
    "\n",
    "        axs[idx, 1].imshow(masks[0].cpu().squeeze(), cmap='gray')\n",
    "        axs[idx, 1].set_title('True Mask')\n",
    "        axs[idx, 1].axis('off')\n",
    "\n",
    "        axs[idx, 2].imshow(preds[0].cpu().squeeze(), cmap='gray')\n",
    "        axs[idx, 2].set_title('Predicted Mask')\n",
    "        axs[idx, 2].axis('off')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "for resize in sizes:\n",
    "    model_path = f'saved_models/model_{resize[0]}x{resize[1]}.pth'\n",
    "    test_loader = create_test_dataloader(base_path, batch_size, resize)\n",
    "    \n",
    "    model = UNet()  # Assume the model has been previously defined\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    iou_scores = []  # List to store IoU scores\n",
    "\n",
    "    for images, true_masks in test_loader:\n",
    "        images = images.to(device)\n",
    "        true_masks = true_masks.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs) > 0.5  # Threshold to obtain binary masks\n",
    "\n",
    "        preds_np = preds.cpu().numpy().astype(np.uint8)\n",
    "        true_masks_np = true_masks.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        # Calculate and store the IoU score for each image in the batch\n",
    "        for pred, true_mask in zip(preds_np, true_masks_np):\n",
    "            iou_score = jaccard_score(true_mask.flatten(), pred.flatten())\n",
    "            iou_scores.append(iou_score)\n",
    "\n",
    "    mean_iou = np.mean(iou_scores)  # Calculate the mean IoU for the current size\n",
    "    print(f'Mean IoU for size {resize}: {mean_iou:.4f}')\n",
    "    plot_examples(test_loader, model, device, num_examples=5)  # Visualize examples of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposal 2: Crop the images into smaller pieces, \\\n",
    "# thus training with more images (these are actually the same images, \n",
    "# but we take small pieces from them, which in practice means we have more material to train with).\n",
    "\n",
    "### The process is the same as before, although most functions need to be adjusted to work with \n",
    "# the new image parameters.\n",
    "\n",
    "class RandomCropTransform:\n",
    "    def __init__(self, output_size):\n",
    "        \"\"\"\n",
    "        Initializes the random cropping transformation with a specified output size.\n",
    "\n",
    "        Args:\n",
    "            output_size (int): Side length of the output square for cropping.\n",
    "        \"\"\"\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        assert image.size[0] >= self.output_size and image.size[1] >= self.output_size\n",
    "        \n",
    "        white_threshold = 0.1  # Threshold for the maximum allowed white pixels in the crop\n",
    "        max_attempts = 100  # Limit the number of attempts to find a suitable crop\n",
    "\n",
    "        for _ in range(max_attempts):\n",
    "            i = random.randint(0, image.size[0] - self.output_size)\n",
    "            j = random.randint(0, image.size[1] - self.output_size)\n",
    "\n",
    "            cropped_image = TF.crop(image, i, j, self.output_size, self.output_size)\n",
    "            cropped_mask = TF.crop(mask, i, j, self.output_size, self.output_size)\n",
    "\n",
    "            image_array = np.array(cropped_image)\n",
    "            num_white_pixels = np.sum(np.all(image_array == [255, 255, 255], axis=-1))\n",
    "            total_pixels = self.output_size * self.output_size\n",
    "            if num_white_pixels / total_pixels < white_threshold:\n",
    "                break\n",
    "                \n",
    "        # Apply additional random transformations\n",
    "        if random.random() > 0.5:\n",
    "            cropped_image, cropped_mask = TF.hflip(cropped_image), TF.hflip(cropped_mask)\n",
    "        if random.random() > 0.5:\n",
    "            cropped_image, cropped_mask = TF.vflip(cropped_image), TF.vflip(cropped_mask)\n",
    "        if random.random() > 0.5:\n",
    "            angle = 90\n",
    "            cropped_image, cropped_mask = TF.rotate(cropped_image, angle), TF.rotate(cropped_mask, angle)\n",
    "\n",
    "        return cropped_image, cropped_mask\n",
    "\n",
    "\n",
    "class SatelliteDataset_onehot(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, output_size, transform=None, resize=True):\n",
    "        \"\"\"\n",
    "        Initializes the satellite dataset with capabilities to transform images and masks.\n",
    "\n",
    "        Args:\n",
    "            image_dir (str): Directory with images.\n",
    "            mask_dir (str): Directory with masks.\n",
    "            output_size (int): Side length of the output square for cropping.\n",
    "            transform (callable, optional): Transformations to apply to the masks.\n",
    "            resize (bool): Whether to apply cropping and resizing.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = [img for img in os.listdir(image_dir) if img.endswith('.png')]\n",
    "        self.transform = transform\n",
    "        self.crop_transform = RandomCropTransform(output_size)\n",
    "        self.resize = resize\n",
    "        self.output_size = output_size\n",
    "        self.resize_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        if self.resize:\n",
    "            image, mask = self.crop_transform(image, mask)\n",
    "            image = self.resize_transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        else:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToOneHot:\n",
    "    def __init__(self, n_classes=2):\n",
    "        \"\"\"\n",
    "        Initializes a transformer that converts masks to one-hot representation.\n",
    "\n",
    "        Args:\n",
    "            n_classes (int): Number of distinct classes in the mask, including the background.\n",
    "        \"\"\"\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def __call__(self, mask):\n",
    "        \"\"\"\n",
    "        Applies the transformation to a mask.\n",
    "\n",
    "        Args:\n",
    "            mask (PIL Image or ndarray): Input mask.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Mask in one-hot format as a PyTorch tensor.\n",
    "        \"\"\"\n",
    "        # Convert the mask to a numpy array and adjust the values to 0 or 1\n",
    "        mask_array = np.array(mask)\n",
    "        mask_array = np.where(mask_array == 255, 1, 0)  # Convert 255 to 1, keeping 0 as is\n",
    "        # Convert the adjusted mask to a tensor\n",
    "        mask_tensor = torch.as_tensor(mask_array, dtype=torch.int64)\n",
    "        # Apply one_hot\n",
    "        one_hot = torch.nn.functional.one_hot(mask_tensor, num_classes=self.n_classes).permute(2, 0, 1).float()\n",
    "        return one_hot\n",
    "\n",
    "def create_dataloaders_onehot(base_path, resize, batch_size=4):\n",
    "    \"\"\"\n",
    "    Creates dataloaders for training and validation with specific transformations, including one-hot for the masks.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Base path where image and mask directories are located.\n",
    "        resize (int): Size to which images and masks will be resized.\n",
    "        batch_size (int): Number of elements in each data batch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two DataLoaders, one for training and another for validation.\n",
    "    \"\"\"\n",
    "    # Transformations for images\n",
    "    transform_image = transforms.Compose([\n",
    "        transforms.Resize(1344),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    transform_mask_train = transforms.Compose([\n",
    "        transforms.Resize(resize),\n",
    "        ToOneHot(),\n",
    "        # transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    transform_mask_val = transforms.Compose([\n",
    "        transforms.Resize(1344),\n",
    "        ToOneHot(),\n",
    "        # transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Data directories\n",
    "    train_dir = os.path.join(base_path, 'train')\n",
    "    train_mask_dir = os.path.join(base_path, 'train_labels')\n",
    "    val_dir = os.path.join(base_path, 'val')\n",
    "    val_mask_dir = os.path.join(base_path, 'val_labels')\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = SatelliteDataset_onehot(train_dir, train_mask_dir, output_size=resize, transform=transform_mask_train, resize=True)\n",
    "    val_dataset = SatelliteBuildingDataset(val_dir, val_mask_dir, transform_image, transform_mask_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, masks, num_images=4):\n",
    "    \"\"\"\n",
    "    Displays a set of images and masks.\n",
    "\n",
    "    Args:\n",
    "        images (Tensor): Tensor of images with dimensions [batch_size, channels, height, width].\n",
    "        masks (Tensor): Tensor of masks with dimensions [batch_size, channels, height, width], \n",
    "                        where each channel represents a different class.\n",
    "        num_images (int): Number of images and masks to show.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(num_images, 2, figsize=(10, 5 * num_images))\n",
    "    for i in range(num_images):\n",
    "        # Reorder the image channels for visualization\n",
    "        img = images[i].permute(1, 2, 0)\n",
    "        img = img.numpy()\n",
    "        # Normalize the image to enhance visualization\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        \n",
    "        # Select the channel corresponding to the class of interest, e.g., buildings\n",
    "        mask = masks[i][0]  # Assuming the second channel represents buildings\n",
    "        mask = mask.numpy()\n",
    "\n",
    "        # Print dimensions to verify correctness\n",
    "        print(\"Image shape:\", images[i].shape, \"Mask shape:\", masks[i].shape)\n",
    "        \n",
    "        axs[i, 0].imshow(img)\n",
    "        axs[i, 1].imshow(mask, cmap='gray')\n",
    "        axs[i, 0].set_title('Satellite Image')\n",
    "        axs[i, 1].set_title('Label Mask (Buildings)')\n",
    "        axs[i, 0].axis('off')\n",
    "        axs[i, 1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming `train_loader` is already defined and loaded properly\n",
    "images, masks = next(iter(train_loader))  # Load a batch of images and masks from the DataLoader\n",
    "show_images(images, masks)  # Call the function to display the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, out_classes=2, up_sample_mode='conv_transpose'):\n",
    "        \"\"\"\n",
    "        Initializes the U-Net with a descending and ascending path, including a final block.\n",
    "\n",
    "        Args:\n",
    "            out_classes (int): Number of output classes for segmentation.\n",
    "            up_sample_mode (str): Method for upsampling; either 'conv_transpose' or 'bilinear'.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        self.up_sample_mode = up_sample_mode\n",
    "        # Initialize the descending blocks\n",
    "        self.down_conv1 = DownBlock(3, 64)\n",
    "        self.down_conv2 = DownBlock(64, 128)\n",
    "        self.down_conv3 = DownBlock(128, 256)\n",
    "        self.down_conv4 = DownBlock(256, 512)\n",
    "        # Bottleneck\n",
    "        self.double_conv = DoubleConv(512, 1024)\n",
    "        # Initialize the ascending blocks\n",
    "        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n",
    "        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n",
    "        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n",
    "        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n",
    "        # Final convolutional layer\n",
    "        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the data flow through the network, using the defined blocks.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input image.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Network outputs with class probabilities.\n",
    "        \"\"\"\n",
    "        x, skip1_out = self.down_conv1(x)\n",
    "        x, skip2_out = self.down_conv2(x)\n",
    "        x, skip3_out = self.down_conv3(x)\n",
    "        x, skip4_out = self.down_conv4(x)\n",
    "        x = self.double_conv(x)\n",
    "        x = self.up_conv4(x, skip4_out)\n",
    "        x = self.up_conv3(x, skip3_out)\n",
    "        x = self.up_conv2(x, skip2_out)\n",
    "        x = self.up_conv1(x, skip1_out)\n",
    "        x = self.conv_last(x)\n",
    "        x = self.softmax(x)  # Apply softmax to the last layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_into_segments(image_tensor, output_size):\n",
    "    \"\"\"\n",
    "    Splits a large image into smaller segments.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (Tensor): Input image.\n",
    "        output_size (int): Size of each square segment.\n",
    "\n",
    "    Returns:\n",
    "        list: List of image segments.\n",
    "        list: List of coordinates corresponding to each segment.\n",
    "    \"\"\"\n",
    "    _, h, w = image_tensor.shape\n",
    "    segments = []\n",
    "    coords = []\n",
    "    for i in range(0, h, output_size):\n",
    "        for j in range(0, w, output_size):\n",
    "            right = min(j + output_size, w)\n",
    "            lower = min(i + output_size, h)\n",
    "            segment = image_tensor[:, i:lower, j:right]\n",
    "            segments.append(segment)\n",
    "            coords.append((j, i))\n",
    "    return segments, coords\n",
    "\n",
    "def predict_and_recombine_segments(segments, coords, model, output_size, image_shape, device):\n",
    "    \"\"\"\n",
    "    Predicts and recombines image segments to form the complete mask.\n",
    "\n",
    "    Args:\n",
    "        segments (list): Image segments.\n",
    "        coords (list): Coordinates of each segment in the original image.\n",
    "        model (UNet): U-Net model.\n",
    "        output_size (int): Size of each segment.\n",
    "        image_shape (tuple): Original dimensions of the image.\n",
    "        device (torch.device): Device on which the model is running.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Complete reconstructed mask.\n",
    "    \"\"\"\n",
    "    full_mask = torch.zeros((2, image_shape[1], image_shape[2]), device=device)\n",
    "    for segment, (x, y) in zip(segments, coords):\n",
    "        segment = segment.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(segment)\n",
    "        full_mask[:, y:y+output_size, x:x+output_size] = output.squeeze(0)\n",
    "    return full_mask\n",
    "\n",
    "def calculate_iou(outputs, masks):\n",
    "    \"\"\"\n",
    "    Calculates the Intersection over Union (IoU) by interpolating the outputs if necessary to match the size of the masks.\n",
    "\n",
    "    Args:\n",
    "        outputs (Tensor): Model output masks.\n",
    "        masks (Tensor): True masks for comparison.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean IoU score.\n",
    "    \"\"\"\n",
    "    if outputs.size()[2:] != masks.size()[2:]:\n",
    "        outputs = F.interpolate(outputs, size=masks.size()[2:], mode='nearest')\n",
    "    outputs = torch.sigmoid(outputs) > 0.5\n",
    "    outputs = outputs.float()\n",
    "    outputs = outputs.cpu().numpy().astype(np.uint8)\n",
    "    masks = masks.cpu().numpy().astype(np.uint8)\n",
    "    ious = [jaccard_score(m.flatten(), o.flatten(), zero_division=1) for m, o in zip(masks, outputs)]\n",
    "    return np.mean(ious)\n",
    "\n",
    "def validate_model(val_loader, model, output_size, device, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a validation dataset.\n",
    "\n",
    "    Args:\n",
    "        val_loader (DataLoader): DataLoader that provides batches of images and masks.\n",
    "        model (nn.Module): Neural network model to evaluate.\n",
    "        output_size (int): Size of the segments into which each image is divided.\n",
    "        device (torch.device): Device where computations are performed (CPU or GPU).\n",
    "        criterion (loss function): Loss function used to evaluate model accuracy.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average validation losses and average IoU scores.\n",
    "    \"\"\"\n",
    "    model.eval()  # Sets the model to evaluation mode (important for deactivating dropout, batchnorm, etc.)\n",
    "    val_losses = []  # List to store losses for each batch\n",
    "    iou_scores = []  # List to store IoU scores for each batch\n",
    "\n",
    "    for images, masks in val_loader:\n",
    "        images = images.to(device)  # Send images to the appropriate device\n",
    "        masks = masks.to(device)  # Send masks to the appropriate device\n",
    "\n",
    "        # Tensor to store predictions for all images in the batch\n",
    "        predictions = torch.zeros(masks.size(0), 2, masks.size(2), masks.size(3), device=device)\n",
    "\n",
    "        # Process each image in the batch individually\n",
    "        for i, image in enumerate(images):\n",
    "            # Split the image into segments and obtain the coordinates of each segment\n",
    "            segments, coords = split_image_into_segments(image.cpu(), output_size)\n",
    "            # Predict and reconstruct the complete mask from the segments\n",
    "            full_mask = predict_and_recombine_segments(segments, coords, model, output_size, image.shape, device)\n",
    "            predictions[i] = full_mask.to(device)  # Save the reconstructed mask in the predictions tensor\n",
    "        \n",
    "        # Calculate loss for the batch using the specified loss function\n",
    "        loss = criterion(predictions, masks.argmax(dim=1))  # Use argmax if masks are in one-hot format\n",
    "        val_losses.append(loss.item())  # Add the batch's loss to the list of losses\n",
    "\n",
    "        # Calculate IoU for the batch using the defined function\n",
    "        iou = calculate_iou(predictions.argmax(dim=1), masks.argmax(dim=1))  # Use argmax to compare predicted classes\n",
    "        iou_scores.append(iou)  # Add the IoU of the batch to the list of IoUs\n",
    "        \n",
    "    # Calculate and return the average of the losses and IoUs of all processed batches\n",
    "    return np.mean(val_losses), np.mean(iou_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_and_segments(data_loader, output_size):\n",
    "    \"\"\"\n",
    "    Visualizes the first image from a DataLoader and its resulting segments of a specified size.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): DataLoader that provides images and masks.\n",
    "        output_size (int): Size of the square segments into which the image will be divided.\n",
    "    \"\"\"\n",
    "    # Get a batch of images and masks\n",
    "    images, masks = next(iter(data_loader))\n",
    "    image = images[0]  # Take the first image from the batch\n",
    "    mask = masks[0]    # Take the first mask from the batch\n",
    "\n",
    "    # Split the image into segments using the defined function\n",
    "    segments, coords = split_image_into_segments(image, output_size)\n",
    "\n",
    "    # Setup for plotting\n",
    "    num_segments = len(segments)\n",
    "    cols = 4  # Number of columns in the plot\n",
    "    rows = (num_segments // cols) + 2  # Calculate number of rows in the plot\n",
    "\n",
    "    plt.figure(figsize=(cols * 4, rows * 4))\n",
    "\n",
    "    # Display the original image in the first subplot\n",
    "    plt.subplot(rows, cols, 1)\n",
    "    plt.imshow(image.permute(1, 2, 0))  # Reorder from [C, H, W] to [H, W, C]\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display each segment in additional subplots\n",
    "    for idx, segment in enumerate(segments, start=2):\n",
    "        plt.subplot(rows, cols, idx)\n",
    "        plt.imshow(segment.permute(1, 2, 0))  # Reorder from [C, H, W] to [H, W, C]\n",
    "        plt.title(f\"Segment {idx-1}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming that `val_loader` and `output_size` are correctly defined\n",
    "visualize_image_and_segments(val_loader, output_size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = UNet()\n",
    "# Check if CUDA is available and move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Adam optimizer with learning rate 0.0001\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [(224, 224), (448, 448)]\n",
    "num_epochs = 5\n",
    "# Assuming you are running this from the main directory of your project\n",
    "base_path = os.getcwd()  # Get the path of the current directory where the script is running\n",
    "base_path = os.path.join(base_path, 'MBDS/png')  # Add 'images' subdirectory to the base path\n",
    "\n",
    "best_loss = 0  # Initialize the best loss as zero for future comparisons\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear CUDA cache to free up GPU memory\n",
    "\n",
    "for resize in sizes:\n",
    "    print(f\"Training model with image size: {resize}\")  # Report the image size to be trained on\n",
    "    train_loader, val_loader = create_dataloaders_onehot(base_path, resize[0])  # Create DataLoader for training and validation\n",
    "\n",
    "    # Reset lists of losses and IoU for each size\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    iou_scores = []\n",
    "\n",
    "    for epoch in range(num_epochs):  # Loop over each epoch\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        train_tqdm = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} Training', leave=True)  # Use tqdm for progress bar\n",
    "        for images, masks in train_tqdm:\n",
    "            images, masks = images.to(device), masks.to(device)  # Move images and masks to the device (GPU or CPU)\n",
    "            \n",
    "            optimizer.zero_grad()  # Reset the optimizer gradients\n",
    "            outputs = model(images)  # Get model outputs\n",
    "            loss = criterion(outputs, masks)  # Calculate loss\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update model weights\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)  # Accumulate loss adjusted for batch size\n",
    "            train_tqdm.set_postfix(loss=(running_loss / (train_tqdm.last_print_n + 1)))  # Display average loss in tqdm\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)  # Calculate average loss for the epoch\n",
    "        train_losses.append(epoch_loss)  # Save training loss\n",
    "\n",
    "        val_loss, average_iou = validate_model(val_loader, model, output_size=1344, device=device, criterion=criterion)  # Validate the model\n",
    "        val_losses.append(val_loss)  # Save validation loss\n",
    "        iou_scores.append(average_iou)  # Save average IoU\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Average IoU: {iou_scores[-1]:.4f}')\n",
    "\n",
    "        if average_iou > best_loss:  # Save the model if the average IoU is the best so far\n",
    "            model_save_path = os.path.join('saved_models', f'model_cropped_{resize[0]}x{resize[1]}.pth')\n",
    "            os.makedirs('saved_models', exist_ok=True)  # Create the directory if it does not exist\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved to {model_save_path}\")\n",
    "            best_loss = average_iou\n",
    "\n",
    "    # Optionally, plot the results for each size\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(f'Losses over Epochs for Resize: {resize}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(iou_scores, label='IoU Score')\n",
    "    plt.title(f'IoU Scores over Epochs for Resize: {resize}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('IoU Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model.apply(weight_reset)  # Reset model weights to avoid residual training effects in the next iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataloader(base_path, batch_size, resize):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for the test dataset with specific transformations for images and masks.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Base path where image and mask directories are stored.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        resize (int): Size to which images and masks will be resized.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: DataLoader object configured for the test dataset.\n",
    "    \"\"\"\n",
    "    # Transformations for images\n",
    "    transform_image = transforms.Compose([\n",
    "        transforms.Resize(1344),  # Resize images to a fixed size\n",
    "        transforms.ToTensor(),  # Convert images to tensors\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the images\n",
    "    ])\n",
    "    \n",
    "    # Transformations for masks, including conversion to one-hot format\n",
    "    transform_mask = transforms.Compose([\n",
    "        transforms.Resize(1344),  # Resize masks to the same size as images\n",
    "        ToOneHot(),  # Convert masks to one-hot format\n",
    "        # Note: Tensor conversion is handled internally by ToOneHot if necessary\n",
    "    ])\n",
    "    \n",
    "    # Directories where the test images and masks are located\n",
    "    test_dir = os.path.join(base_path, 'test')\n",
    "    test_mask_dir = os.path.join(base_path, 'test_labels')\n",
    "    # Create a dataset using the defined transformations\n",
    "    test_dataset = SatelliteBuildingDataset(test_dir, test_mask_dir, transform_image, transform_mask)\n",
    "    # Create the DataLoader with the test data\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return test_loader\n",
    "\n",
    "def test_and_plot_model(val_loader, model, output_size, device, num_examples=1):\n",
    "    \"\"\"\n",
    "    Evaluates the model and visualizes the results for a limited set of examples from the validation DataLoader.\n",
    "\n",
    "    Args:\n",
    "        val_loader (DataLoader): DataLoader that provides test or validation data.\n",
    "        model (torch.nn.Module): Neural network model to be evaluated.\n",
    "        output_size (int): Desired output size for the image segments.\n",
    "        device (torch.device): Device on which the model is running (CPU or GPU).\n",
    "        num_examples (int): Number of examples to visualize.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    for batch_idx, (images, masks) in enumerate(val_loader):\n",
    "        if batch_idx >= num_examples:\n",
    "            break\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        predictions = torch.zeros_like(masks)  # Prepare tensor to store predictions\n",
    "\n",
    "        for i, (image, mask) in enumerate(zip(images, masks)):\n",
    "            segments, coords = split_image_into_segments(image.cpu(), output_size)\n",
    "            full_mask = predict_and_recombine_segments(segments, coords, model, output_size, image.shape, device)\n",
    "            predictions[i] = full_mask.to(device)\n",
    "        \n",
    "            # Convert one-hot masks to a single channel for visualization\n",
    "            mask_single_channel = torch.argmax(mask, dim=0).cpu()\n",
    "            full_mask_single_channel = torch.argmax(full_mask, dim=0).cpu()\n",
    "        \n",
    "            # Setup plots to show the original image, the original mask, and the predicted mask\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(image.cpu().permute(1, 2, 0))\n",
    "            plt.title(\"Original Image\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(mask_single_channel, cmap=\"gray\")\n",
    "            plt.title(\"Original Mask\")\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(full_mask_single_channel, cmap=\"gray\")\n",
    "            plt.title(\"Predicted Full Mask\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1  # Batch size set to 1, useful for detailed evaluations or where individual processing is required\n",
    "current_directory = os.getcwd()  # Assuming 'current_directory' is defined somewhere else in the code\n",
    "base_path = os.path.join(current_directory, 'MBDS/png')\n",
    "\n",
    "# Loop over different image sizes to test the model with each size configuration\n",
    "for resize in sizes:\n",
    "    model_path = f'saved_models/model_cropped_{resize[0]}x{resize[1]}.pth'  # Path to the saved model\n",
    "    test_loader = create_test_dataloader(base_path, batch_size, resize)  # Create DataLoader for the test set\n",
    "\n",
    "    model = UNet()  # Creation of the model instance, assuming it is a U-Net\n",
    "    model.load_state_dict(torch.load(model_path))  # Load the model weights from the saved file\n",
    "    model.to(device)  # Move the model to the appropriate device (GPU or CPU)\n",
    "\n",
    "    # Run the function that tests the model and visualizes the results\n",
    "    # Note: Ensure that 'output_size' is correctly defined here to match the logic of 'test_and_plot_model'\n",
    "    test_and_plot_model(test_loader, model, resize[0], device, num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results with this method are much better, improving the original result by fivefold in terms of IoU. \n",
    "# To further improve the model, I think it could be modified to use some type of pre-trained model to constitute the U-Net network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coen6331",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
